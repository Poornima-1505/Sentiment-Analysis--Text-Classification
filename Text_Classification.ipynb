{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Poornima\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Classify_text_data.csv' does not exist: b'Classify_text_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-22a24edf5684>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mText\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Classify_text_data.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The dimensions of the dataset'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_1\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_1\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_1\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_1\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_1\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Classify_text_data.csv' does not exist: b'Classify_text_data.csv'"
     ]
    }
   ],
   "source": [
    "Text=pd.read_csv('Classify_text_data.csv',names=['message','label'])\n",
    "print('The dimensions of the dataset',Text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the dataset (100, 2)\n"
     ]
    }
   ],
   "source": [
    "Text=pd.read_csv(\"C:/docs/Predictive/Classify_text_data.csv\",names=['message','label'])\n",
    "print('The dimensions of the dataset',Text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                      Life is great\n",
      "1                                        I am strong\n",
      "2                                       I am healthy\n",
      "3                                      I am powerful\n",
      "4                              I am dreadful fertile\n",
      "                           ...                      \n",
      "95                                     I’m a failure\n",
      "96                                      I am too old\n",
      "97    I need to wait until I get everything together\n",
      "98                              I’m not smart enough\n",
      "99                                      I can’t play\n",
      "Name: message, Length: 100, dtype: object\n",
      "0     1\n",
      "1     1\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "     ..\n",
      "95    0\n",
      "96    0\n",
      "97    0\n",
      "98    0\n",
      "99    0\n",
      "Name: labelnum, Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Text['labelnum']=Text.label.map({'pos':1,'neg':0})\n",
    "X=Text.message\n",
    "Y=Text.labelnum\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25,)\n",
      "(75,)\n",
      "(25,)\n",
      "(75,)\n"
     ]
    }
   ],
   "source": [
    "print(xtest.shape)\n",
    "print(xtrain.shape)\n",
    "print(ytest.shape)\n",
    "print(ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandon', 'able', 'achieved', 'alice', 'alive', 'all', 'always', 'am', 'an', 'and', 'ann', 'anyways', 'are', 'as', 'at', 'attractive', 'been', 'being', 'best', 'bill', 'body', 'brother', 'brothers', 'can', 'cant', 'care', 'cats', 'chase', 'cheat', 'child', 'chinese', 'clay', 'come', 'company', 'conscious', 'cry', 'crying', 'day', 'did', 'do', 'does', 'dogs', 'doing', 'dollars', 'don', 'dont', 'down', 'dreadful', 'drink', 'earn', 'eat', 'else', 'enjoy', 'esther', 'every', 'everything', 'exactly', 'expected', 'experiences', 'failure', 'faking', 'feel', 'feeling', 'fell', 'fertile', 'fiction', 'fine', 'foods', 'for', 'french', 'from', 'fullfilled', 'get', 'go', 'good', 'grass', 'great', 'happy', 'has', 'hasn', 'hating', 'have', 'having', 'he', 'healthy', 'helping', 'here', 'him', 'home', 'hundred', 'hurts', 'husband', 'if', 'ignore', 'in', 'inner', 'insurance', 'intensive', 'invisible', 'invited', 'is', 'it', 'john', 'knowing', 'knows', 'last', 'learned', 'letting', 'life', 'like', 'lions', 'listen', 'little', 'live', 'lively', 'living', 'love', 'made', 'make', 'makes', 'me', 'models', 'morning', 'much', 'my', 'myself', 'nature', 'needs', 'never', 'not', 'now', 'of', 'old', 'on', 'our', 'patient', 'people', 'person', 'pete', 'play', 'playing', 'powerful', 'practice', 'quick', 'read', 'right', 'science', 'self', 'serene', 'sharon', 'she', 'should', 'show', 'sick', 'sleep', 'smile', 'so', 'some', 'somewhere', 'soul', 'speak', 'special', 'spicy', 'store', 'strong', 'talk', 'team', 'tests', 'thank', 'thankful', 'thanks', 'that', 'the', 'they', 'this', 'time', 'to', 'today', 'tomorrow', 'too', 'trust', 'two', 'type', 'unwanted', 'used', 'very', 'violin', 'voice', 'waiting', 'want', 'way', 'we', 'week', 'well', 'what', 'when', 'who', 'will', 'win', 'wine', 'with', 'woman', 'wont', 'work', 'worst', 'yesterday', 'you', 'young']\n",
      "['abandon', 'able', 'achieved', 'alice', 'alive', 'all', 'always', 'am', 'an', 'and', 'ann', 'anyways', 'are', 'as', 'at', 'attractive', 'been', 'being', 'best', 'bill', 'body', 'brother', 'brothers', 'can', 'cant', 'care', 'cats', 'chase', 'cheat', 'child', 'chinese', 'clay', 'come', 'company', 'conscious', 'cry', 'crying', 'day', 'did', 'do', 'does', 'dogs', 'doing', 'dollars', 'don', 'dont', 'down', 'dreadful', 'drink', 'earn', 'eat', 'else', 'enjoy', 'esther', 'every', 'everything', 'exactly', 'expected', 'experiences', 'failure', 'faking', 'feel', 'feeling', 'fell', 'fertile', 'fiction', 'fine', 'foods', 'for', 'french', 'from', 'fullfilled', 'get', 'go', 'good', 'grass', 'great', 'happy', 'has', 'hasn', 'hating', 'have', 'having', 'he', 'healthy', 'helping', 'here', 'him', 'home', 'hundred', 'hurts', 'husband', 'if', 'ignore', 'in', 'inner', 'insurance', 'intensive', 'invisible', 'invited', 'is', 'it', 'john', 'knowing', 'knows', 'last', 'learned', 'letting', 'life', 'like', 'lions', 'listen', 'little', 'live', 'lively', 'living', 'love', 'made', 'make', 'makes', 'me', 'models', 'morning', 'much', 'my', 'myself', 'nature', 'needs', 'never', 'not', 'now', 'of', 'old', 'on', 'our', 'patient', 'people', 'person', 'pete', 'play', 'playing', 'powerful', 'practice', 'quick', 'read', 'right', 'science', 'self', 'serene', 'sharon', 'she', 'should', 'show', 'sick', 'sleep', 'smile', 'so', 'some', 'somewhere', 'soul', 'speak', 'special', 'spicy', 'store', 'strong', 'talk', 'team', 'tests', 'thank', 'thankful', 'thanks', 'that', 'the', 'they', 'this', 'time', 'to', 'today', 'tomorrow', 'too', 'trust', 'two', 'type', 'unwanted', 'used', 'very', 'violin', 'voice', 'waiting', 'want', 'way', 'we', 'week', 'well', 'what', 'when', 'who', 'will', 'win', 'wine', 'with', 'woman', 'wont', 'work', 'worst', 'yesterday', 'you', 'young']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()\n",
    "xtrain_dt = count_vector.fit_transform(xtrain)\n",
    "xtest_dt=count_vector.transform(xtest)\n",
    "print(count_vector.get_feature_names())\n",
    "Words=count_vector.get_feature_names()\n",
    "print(Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    abandon  able  achieved  alice  alive  all  always  am  an  and  ...  win  \\\n",
      "0         0     0         0      0      0    0       0   2   0    0  ...    0   \n",
      "1         0     0         0      0      0    0       0   0   0    0  ...    0   \n",
      "2         0     0         0      0      0    0       0   0   0    0  ...    0   \n",
      "3         0     0         0      0      0    0       2   0   0    1  ...    0   \n",
      "4         0     0         0      0      0    0       0   0   0    0  ...    0   \n",
      "..      ...   ...       ...    ...    ...  ...     ...  ..  ..  ...  ...  ...   \n",
      "70        0     0         0      0      0    0       0   0   0    0  ...    0   \n",
      "71        0     0         0      0      0    0       0   1   0    0  ...    0   \n",
      "72        0     0         0      0      0    0       0   0   0    0  ...    0   \n",
      "73        0     0         0      0      0    0       0   0   0    0  ...    0   \n",
      "74        0     0         0      0      0    0       0   1   0    0  ...    0   \n",
      "\n",
      "    wine  with  woman  wont  work  worst  yesterday  you  young  \n",
      "0      0     0      0     0     0      0          0    0      0  \n",
      "1      0     0      0     0     0      0          0    0      0  \n",
      "2      0     0      0     0     0      0          0    0      0  \n",
      "3      0     0      0     0     0      0          0    0      0  \n",
      "4      0     0      0     0     0      0          0    0      0  \n",
      "..   ...   ...    ...   ...   ...    ...        ...  ...    ...  \n",
      "70     0     0      0     0     0      0          0    0      0  \n",
      "71     0     0      0     0     0      0          0    0      0  \n",
      "72     0     0      0     0     0      0          0    0      0  \n",
      "73     0     0      0     0     0      0          0    0      0  \n",
      "74     0     0      0     0     0      0          0    0      0  \n",
      "\n",
      "[75 rows x 208 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(xtrain_dt.toarray(),columns=count_vector.get_feature_names())\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(xtrain_dt,ytrain)\n",
    "prediction = clf.predict(xtest_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy metrics\n",
      "Accuracy of the classifer is 0.96\n",
      "Confusion matrix\n",
      "[[14  1]\n",
      " [ 0 10]]\n",
      "Recall and Precison \n",
      "1.0\n",
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('Accuracy metrics')\n",
    "print('Accuracy of the classifer is',metrics.accuracy_score(ytest,prediction))\n",
    "print('Confusion matrix')\n",
    "print(metrics.confusion_matrix(ytest,prediction))\n",
    "print('Recall and Precison ')\n",
    "print(metrics.recall_score(ytest,prediction))\n",
    "print(metrics.precision_score(ytest,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e8eb59de0ca9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading https://files.pythonhosted.org/packages/65/41/abefdda082c7b211248e412fb2c7a8dc69d474f18ed61a5a784f20f73bb7/wordcloud-1.7.0-cp37-cp37m-win_amd64.whl (157kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\poornima\\anaconda3_1\\lib\\site-packages (from wordcloud) (3.1.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\poornima\\anaconda3_1\\lib\\site-packages (from wordcloud) (1.16.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\poornima\\anaconda3_1\\lib\\site-packages (from wordcloud) (6.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\poornima\\anaconda3_1\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\poornima\\anaconda3_1\\lib\\site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\poornima\\anaconda3_1\\lib\\site-packages (from matplotlib->wordcloud) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\poornima\\anaconda3_1\\lib\\site-packages (from matplotlib->wordcloud) (2.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\poornima\\anaconda3_1\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\poornima\\anaconda3_1\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (41.4.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-4e0961807dd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mbackground_color\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'white'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mWordCloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 min_font_size = 10).generate(comment_words) \n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# plot the WordCloud image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'set' object is not callable"
     ]
    }
   ],
   "source": [
    "comment_words = '' \n",
    "WordCloud = set(Words) \n",
    "for val in xtrain_dt: \n",
    "      \n",
    "    # typecaste each val to string \n",
    "    val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "    tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "      \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                WordCloud = Words, \n",
    "                min_font_size = 10).generate(comment_words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
